{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUivj4I28bLqXcTMu8Z3n6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JNAbhishek27/DataScope/blob/main/DataScope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pandas requests beautifulsoup4 pyngrok -q\n"
      ],
      "metadata": {
        "id": "MfkWvC084BuB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"headline\": [\n",
        "        \"COVID-19 vaccines contain microchips\",\n",
        "        \"NASA confirms water on the Moon\",\n",
        "        \"Drinking bleach cures coronavirus\",\n",
        "        \"Apple launches new iPhone with satellite connectivity\",\n",
        "        \"Climate change is a hoax\",\n",
        "        \"UN reports record global temperatures in 2023\",\n",
        "        \"Bill Gates wants to control population with vaccines\",\n",
        "        \"WHO declares end of COVID-19 as a global health emergency\",\n",
        "        \"Elon Musk builds colony on Mars in 2025\",\n",
        "        \"OpenAI releases new AI model GPT-5\"\n",
        "    ],\n",
        "    \"label\": [\n",
        "        \"fake\", \"real\", \"fake\", \"real\", \"fake\",\n",
        "        \"real\", \"fake\", \"real\", \"fake\", \"real\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"headlines.csv\", index=False)\n",
        "print(\"‚úÖ headlines.csv created in Colab\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P0UnNcg7_hj",
        "outputId": "001de5df-e7c8-4d4d-c6e5-83ef7a5764c0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ headlines.csv created in Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pandas requests beautifulsoup4 duckduckgo-search -q\n",
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Q8gcYdRQdq",
        "outputId": "3ce9bdb2-4715-4da7-f20f-86c83c428773"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K\n",
            "changed 22 packages in 2s\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests, json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "ELASTIC_API_KEY = \"ENTER_YOUR_KEY\"\n",
        "ELASTIC_ENDPOINT = \"ELASTIC_ENDPOINT\"\n",
        "\n",
        "# Serper API (Google Search alternative)\n",
        "SERPER_API_KEY = \"SERPER_API_KEY\"\n",
        "SERPER_ENDPOINT = \"SERPER_ENDPOINT\"\n",
        "\n",
        "# -----------------------------\n",
        "# Helper functions\n",
        "# -----------------------------\n",
        "def search_elastic(query):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"ApiKey {ELASTIC_API_KEY}\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"query\": {\n",
        "            \"multi_match\": {\n",
        "                \"query\": query,\n",
        "                \"fields\": [\"headline\", \"content\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(ELASTIC_ENDPOINT, headers=headers, data=json.dumps(payload), timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            hits = response.json().get(\"hits\", {}).get(\"hits\", [])\n",
        "            if hits:\n",
        "                top_hit = hits[0][\"_source\"]\n",
        "                return top_hit.get(\"headline\", \"\"), top_hit.get(\"label\", \"unknown\"), hits[0].get(\"_score\", 0)\n",
        "    except:\n",
        "        pass\n",
        "    return None, None, 0\n",
        "\n",
        "def search_serper(query):\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\"q\": query}\n",
        "    try:\n",
        "        response = requests.post(SERPER_ENDPOINT, headers=headers, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            results = response.json().get(\"organic\", [])\n",
        "            return [r.get(\"title\") for r in results[:5]]\n",
        "    except:\n",
        "        pass\n",
        "    return []\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "        return soup.title.string.strip() if soup.title else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# Streamlit UI\n",
        "# -----------------------------\n",
        "st.set_page_config(page_title=\"DataScope 2030\", page_icon=\"üîç\", layout=\"centered\")\n",
        "st.title(\"üîç DataScope 2030\")\n",
        "st.write(\"Check if a news headline or article link is Availabel using RAG (Elastic + Web Search)!\")\n",
        "\n",
        "input_option = st.radio(\"Choose input type:\", [\"Headline\", \"URL\"])\n",
        "\n",
        "user_input = None\n",
        "if input_option == \"Headline\":\n",
        "    user_input = st.text_area(\"‚úçÔ∏è Enter a news headline:\")\n",
        "elif input_option == \"URL\":\n",
        "    user_url = st.text_input(\"üåç Enter a news article link:\")\n",
        "    if user_url:\n",
        "        extracted_text = extract_text_from_url(user_url)\n",
        "        if extracted_text:\n",
        "            st.write(f\"üì∞ Extracted Headline: *{extracted_text}*\")\n",
        "            user_input = extracted_text\n",
        "        else:\n",
        "            st.error(\"Couldn't extract text from URL\")\n",
        "\n",
        "if st.button(\"Check Data\") and user_input:\n",
        "    # 1. Check Elastic index\n",
        "    headline, label, score = search_elastic(user_input)\n",
        "\n",
        "    if headline:\n",
        "        status = \"‚úÖ Likely True\" if label == \"real\" else \"‚ùå Suspicious\"\n",
        "        st.metric(\"Elastic Truth Score\", f\"{round(score*20,2)}%\")\n",
        "        st.write(f\"Match found: *{headline}* ‚Üí {status}\")\n",
        "    else:\n",
        "        # 2. Fallback to Serper web search\n",
        "        web_results = search_serper(user_input)\n",
        "        if web_results:\n",
        "            st.metric(\"Found in web\", f\"{len(web_results)} sources\")\n",
        "            st.write(\"Closest matches from web:\")\n",
        "            for r in web_results:\n",
        "                st.caption(f\"- {r}\")\n",
        "        else:\n",
        "            st.warning(\"‚ùì Not enough info found anywhere\")\n",
        "def rag_check_headline(headline):\n",
        "    \"\"\"\n",
        "    RAG-style check: use Serper API or Google Search to see if headline is supported online.\n",
        "    Returns status (True/False), confidence, and top source.\n",
        "    \"\"\"\n",
        "    import requests\n",
        "\n",
        "    # Example: Using Serper API (replace YOUR_SERPER_API_KEY)\n",
        "    API_KEY = \"YOUR_SERPER_API_KEY\"\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    headers = {\"X-API-KEY\": API_KEY}\n",
        "    data = {\"q\": headline, \"num\": 3} project\n",
        "\n",
        "    try:\n",
        "        resp = requests.post(url, headers=headers, json=data, timeout=5)\n",
        "        resp.raise_for_status()\n",
        "        results = resp.json().get(\"organic\", [])\n",
        "        if results:\n",
        "            top_result = results[0]\n",
        "            link = top_result.get(\"link\", \"\")\n",
        "            snippet = top_result.get(\"snippet\", \"\")\n",
        "            # simple heuristic: if headline words appear in top snippet ‚Üí Likely True\n",
        "            match_count = sum([1 for w in headline.lower().split() if w in snippet.lower()])\n",
        "            confidence = int((match_count / len(headline.split())) * 100)\n",
        "            status = \"‚úÖ Likely True\" if confidence > 30 else \"‚ùå Suspicious\"\n",
        "            return status, confidence, link\n",
        "    except:\n",
        "        pass\n",
        "    return \"‚ùì Not enough info\", 0, None\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjAfyvwiRVmm",
        "outputId": "9cfebdef-d8e0-4519-af6b-34b121da778b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "# from google.colab import userdata # Uncomment this line if using secrets\n",
        "\n",
        "# Kill any running ngrok processes\n",
        "for proc in psutil.process_iter(['pid', 'name']):\n",
        "    if proc.info['name'] == 'ngrok':\n",
        "        print(f\"Killing ngrok process with PID: {proc.info['pid']}\")\n",
        "        proc.kill()\n",
        "\n",
        "\n",
        "# Run Streamlit in the background\n",
        "os.system(\"nohup streamlit run app.py --server.port 8501 &\")\n",
        "\n",
        "time.sleep(5)  # Give Streamlit a few seconds to start\n",
        "\n",
        "# Set ngrok authtoken - replace \"YOUR_AUTH_TOKEN\" with your actual authtoken\n",
        "# ngrok.set_auth_token(userdata.get('NGROK_AUTH_TOKEN')) # Uncomment this line if using secrets\n",
        "ngrok.set_auth_token(\"NGROK_AUTH_TOKEN\") # Replace with your actual authtoken\n",
        "\n",
        "\n",
        "# Open Ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üåê Your DataScope 2030 app is live at:\", public_url)"
      ],
      "metadata": {
        "id": "6A6eNiV99vhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b7231ee6-981d-4c0f-f236-9b158a8ac044"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Killing ngrok process with PID: 3470\n",
            
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUgWKQEUL2mV"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}
